{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CharlieOlives/MAI/blob/main/linguisticsai_practical2_2324.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xbfjqzethn7k"
      },
      "source": [
        "# Linguistics and artificial intelligence: practical session 2 (24 October 2023)\n",
        "\n",
        "Note: you are not required to submit any assignments for this practical session.\n",
        "\n",
        "For these exercises, we will make use of the interactive Python environment provided by Google Colab. The environment makes it possible to run Python code within your web browser. It will be helpful if you already know some basic Python (as provided in the first lectures of the course 'Scripting Languages', for example). But even you don't know anything about programming, you should be able to follow along with the examples.\n",
        "\n",
        "You are highly recommended to use Chrome as a browser to run this notebook. In other browers (firefox, safari, ..), you are likely to run into problems with certain parts (such as the visualization interface below).\n",
        "\n",
        "## Word embeddings\n",
        "\n",
        "In this practical session, we'll automatically induce word embeddings based on an extract from Wikipedia. To do so, we'll make use of the Python library called `gensim`. `gensim` is a vector space modeling and topic modeling\n",
        "toolkit for Python, and it contains an efficient implementation of\n",
        "the word2vec algorithms for word embedding induction.\n",
        "\n",
        "Word2vec consists of two different algorithms: skipgram (sg) and\n",
        "continuous-bag-of-words (cbow). The underlying prediction task of the\n",
        "former is to estimate the context words from the target word; the\n",
        "prediction task of the latter is to estimate the target word from the\n",
        "sum of the context words. More information can be found in the course\n",
        "slides and in the background material of the course.\n",
        "\n",
        "We'll import the Word2Vec class from the `gensim` library using the command below. We'll also import the `logging` module, which will provide us with information during training, and the `gzip` module, which allows us to read compressed files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5oVGN8oBDoP"
      },
      "source": [
        "import logging\n",
        "import gzip\n",
        "\n",
        "from gensim.models import Word2Vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUUv92RgxCLB"
      },
      "source": [
        "The following command will download a sample from the English Wikipedia, consisting of 1 million lines (about 26 million words). Note that our corpus has already been tokenized (i.e. punctuation marks have been separated from the words)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVRaffrQBEl6"
      },
      "source": [
        "!wget http://ccl.kuleuven.be/Courses/linguistics_and_ai/data/wiki_sample_en.txt.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmMmB4QF5BhW"
      },
      "source": [
        "We can inspect the beginning of our corpus using the command below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZ6WxqC45CXE"
      },
      "source": [
        "!zcat wiki_sample_en.txt.gz|head -n20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4123jcocxV61"
      },
      "source": [
        "The next command initializes the logging module, which will provide information about our training procedure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmdA74Y4DRow"
      },
      "source": [
        "#remove any default config\n",
        "for handler in logging.root.handlers[:]:\n",
        "   logging.root.removeHandler(handler)\n",
        "\n",
        "#initialize logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\n",
        "                    level=logging.INFO)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCtT2XlHxeru"
      },
      "source": [
        "Next, we define a corpus class that will allow us to sequentially iterate over the sentences in our corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPgb9KyqCy9T"
      },
      "source": [
        "class PlainTextCorpus(object):\n",
        "    def __init__(self, fileName):\n",
        "        self.fileName = fileName\n",
        "\n",
        "    def __iter__(self):\n",
        "        for line in gzip.open(self.fileName, 'rt'):\n",
        "            line = [w.lower() for w in line.split()]\n",
        "            yield line"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dW1Yiwvyx0tG"
      },
      "source": [
        "And we instantiate that class using the location of our corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiFoMNQjC7mK"
      },
      "source": [
        "sentences = PlainTextCorpus('wiki_sample_en.txt.gz')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xV66y_8ByX1C"
      },
      "source": [
        "We're all set to start our training procedure, using the command below. We'll go through our corpus, and induce embeddings with a vector size of 100 (`gensim`'s default). In order to have reasonable estimates, we'll only take into account words that appear with a frequency of at least 250 in our corpus. We'll sequentially go through our corpus 5 times (in neural network lingo, one traversal is called an *epoch*). Note that our corpus is fairly small; we'd typically use data that is one to two orders of magnitude larger than the toy data we're using here. Still, the corpus will allow us to come up with reasonable word embedding representations.\n",
        "\n",
        "The following command will take a couple of minutes to finish, so sit back and enjoy watching the training procedure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNXFC2APDHNo"
      },
      "source": [
        "model = Word2Vec(sentences, min_count=250)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYFm4Lr5zrM3"
      },
      "source": [
        "Once our training procedure is finished, we'll have a model that contains embeddings for a vocabulary of 8120 words, as can be confirmed with the command below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7p8AeKaaDLqO"
      },
      "source": [
        "len(model.wv.index_to_key)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5BK0Km1zyNi"
      },
      "source": [
        "It is now straightforward to compute the most similar words according to our embedding model. Use the command below to compute the most similar words to the word *piano*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbbG6CO_DkUy"
      },
      "source": [
        "model.wv.most_similar('piano')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKRzu45A0MlF"
      },
      "source": [
        "During the lecture, we saw that you can do analogy computations by carrying out vector arithmetic. Let's try that out. Note that the `Word2Vec` module encodes the computations slightly differently. For a computation like *brussels - belgium + france*, you need to provide as arguments a list of the positive terms (*brussels* and *france*), and a list of the negative terms (*belgium*)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjQBEj2maWDT"
      },
      "source": [
        "model.wv.most_similar(positive=['brussels', 'france'], negative=['belgium'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLFPETsF1M72"
      },
      "source": [
        "Let's also try out the ubiquitous example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XS8Or3xoi1Ko"
      },
      "source": [
        "model.wv.most_similar(positive=['king', 'woman'], negative=['man'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***note:*** Neural networks are non-deterministic, so my output is different than that of other people. Is because model trains slightly different. Multiple epochs helps when you have little data."
      ],
      "metadata": {
        "id": "24bYZyJp_Zfk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcRYEAvP2e_l"
      },
      "source": [
        "Finally, we'll explore a visualization of the embeddings we created. To do so, we'll make use of the `TensorBoard Projector`, the visualization module that comes with Google's `Tensorflow` library for neural network modeling. First we'll import the necessary modules (including the `os` module, which allows us to perform some system commands)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mh22cCoM8t7e"
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorboard.plugins import projector\n",
        "\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNhP3aU63muP"
      },
      "source": [
        "We can now convert our embeddings into the right format, so that they can be used by `TensorBoard`. If you're interested in how that works, you can inspect the comments provided in the code below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pi8_SCYRdn9x"
      },
      "source": [
        "# set up a logs directory for Tensorboard\n",
        "log_dir='/logs/embeddings/'\n",
        "if not os.path.exists(log_dir):\n",
        "    os.makedirs(log_dir)\n",
        "\n",
        "# save vocabulary line by line\n",
        "with open(os.path.join(log_dir, 'metadata.tsv'), \"w\") as f:\n",
        "  for w in model.wv.index_to_key:\n",
        "    f.write(\"{}\\n\".format(w))\n",
        "\n",
        "# save the embedding vectors in a variable\n",
        "weights = tf.Variable(model.wv.get_normed_vectors())\n",
        "\n",
        "# create a checkpoint for the embeddings\n",
        "checkpoint = tf.train.Checkpoint(embedding=weights)\n",
        "checkpoint.save(os.path.join(log_dir, \"embedding.ckpt\"))\n",
        "\n",
        "# Some more configuration setup\n",
        "config = projector.ProjectorConfig()\n",
        "embedding = config.embeddings.add()\n",
        "embedding.tensor_name = \"embedding/.ATTRIBUTES/VARIABLE_VALUE\"\n",
        "embedding.metadata_path = 'metadata.tsv'\n",
        "projector.visualize_embeddings(log_dir, config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWug7pj735RM"
      },
      "source": [
        "We're all set. We can now load `TensorBoard`, and use the `Projector` to explore the embeddings we created.\n",
        "\n",
        "Note: if you get the error `No dashboards are active for the current dataset`, this means the interface has not yet properly loaded. Just press the play button for the same command again in order to properly load it.\n",
        "\n",
        "Function of running epochs: voor woorden met verschillende betekenissen maar zelfde schrijfwijze (bank bijvoorbeeld), iedere keer dat het netwerk deze tegenkomt ziet hij deze anders. Vb 1 keer als 'geld', andere keer als 'zetel'. Iedere extra keer past hij de vector van het woord bank aan zodat deze beter de twee betekenissen kan aannemen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtL_KzYMBIzP"
      },
      "source": [
        "# Run tensorboard with the log data\n",
        "%tensorboard --logdir /logs/embeddings/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEXX5KE6j6Np"
      },
      "source": [
        "The embeddings you have trained will now be displayed in a 3D visualization in the `TensorBoard Projector`. Using the panel on the right, you can search for words to find the closest neighbors to a particular word. For example, try searching for *basketball*. You are likely to see similar neighbors such as *soccer* and *football*. Note that you can use your mouse to move through the 3D space, and click on the circles to visualize the neighbors of a particular word in space.\n",
        "\n",
        "The visualization you're seeing is a projection of your original embedding space (of 100 dimensions) to three dimensions, using a dimensionality reduction technique called Principal Component Analysis (PCA). The technique projects the original space down to three dimensions, preserving as much of the variance in the original space as possible.\n",
        "\n",
        "### Exercise 1\n",
        "\n",
        "Try searching for ambiguous words, such as *record* or *sentence*. What does the neighbouring space look like? Can you see clusters of related senses? You can click the `Isolate .. points` button in the top right in order to visualize only the nearest neighbours, in order to gain a clearer view of the neighbouring space. Also try out a different dimensionality reduction algorithm, such as `UMAP` (this is also a dimensionality reduction technique, but works a bit different: it tries to preserve the nearest neighbours within the original space as closely as possible; you could also try `t-SNE` - which uses a similar idea - but this technique is quite a bit slower, so be prepared to wait).\n",
        "\n",
        "### Exercise 2\n",
        "\n",
        "By default, the `Word2Vec` class within the `gensim` library uses *continuous bag of words* (cbow) to induce the embeddings. Change the arguments, so that your embeddings are constructed using the *skip gram* method. You can find more information about the `Word2Vec` class (including the possible arguments) in `gensim`'s [API reference](https://radimrehurek.com/gensim/models/word2vec.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Solution exercise 1:***\n",
        "\n",
        "Run it in FireFox for the visualization. Doesn't work on Chrome. Select 'Projector' for visualization.\n",
        "\n",
        "***Solution exercise 2:***\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FHvfJLN_84n9"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQI_tRcKrN91"
      },
      "source": [
        "## YOUR CODE HERE: zie oplossing! TA stond ervoor\n",
        "model = Word2VEc()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}