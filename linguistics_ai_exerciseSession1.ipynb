{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CharlieOlives/MAI/blob/main/linguistics_ai_exerciseSession1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xW9VnP9cZMkk"
      },
      "source": [
        "# Linguistics and artificial intelligence: practical session 1 (10 October 2023)\n",
        "\n",
        "## Machine learning for NLP\n",
        "\n",
        "\n",
        "Note: you are not required to submit any assignments for this practical session.\n",
        "\n",
        "In this practical session, you will carry out a number of programming\n",
        "exercises on machine learning for NLP, using Python. It will be helpful if you already know some basic Python (as provided in the first lectures of the course 'Scripting Languages', for example). But even you don't know anything about programming, you should be able to follow along with the examples.\n",
        "\n",
        "For these exercises, we will make use of the interactive Python environment provided by Google Colab. The environment makes it possible to run Python code within your web browser. As an example, the code below computes the result of a sum, and outputs the result. Run the code by hovering over it, and pressing the play button in front.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riRm8LteA4vA"
      },
      "source": [
        "result = 2 + 2\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction\n",
        "\n",
        "In this practical session, we will explore machine learning models for\n",
        "NLP applications; specifically, we will train statistical and neural network classifiers for\n",
        "sentiment analysis on an English dataset of movie reviews (Maas et al., 2011).\n",
        "\n",
        "First, download the archive that contains the dataset and unpack it, using the commands below:"
      ],
      "metadata": {
        "id": "qwvVARxQorP7"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKywkSax3Z2I"
      },
      "source": [
        "!wget http://www.ccl.kuleuven.be/Courses/linguistics_and_ai/data/review_dataset.tar.gz\n",
        "!tar xzvf review_dataset.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZFTcEY_aQpf"
      },
      "source": [
        "A traditional machine learning pipeline for NLP applications generally consists of the following stages:\n",
        "\n",
        "* data preprocessing (tokenization)\n",
        "* feature extraction\n",
        "* model training\n",
        "* evaluation\n",
        "\n",
        "\n",
        "We'll go through these stages step by step, using sentiment\n",
        "classification as an application. As a dataset, we'll be using a set\n",
        "of movie reviews in English, extracted from the website\n",
        "[imdb.com](https://www.imdb.com). The dataset consists of the text of the review, as\n",
        "well as a sentiment label (positive or negative). Note that\n",
        "  the original ratings on IMDB range from 0 to 10\n",
        "  stars. We will use binary classification instead. In our dataset,\n",
        "  original reviews of $\\leq 4$ stars are considered negative, while\n",
        "  reviews of $\\geq 7$ stars are considered positive. The training set is\n",
        "divided into a training part (for training, 20000 reviews, i.e. 80%) and\n",
        "test part (for evaluation, 5000 reviews, i.e. 20%). The dataset is\n",
        "balanced, which means positive and negative instances are evenly\n",
        "distributed in both train and test set.\n",
        "\n",
        "### Preprocessing\n",
        "\n",
        "First, we'll load the training set. To do so, we'll make use of the `pandas` library, which facilitates data manipulation. Issue the commands below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uy4PR9K3jSi"
      },
      "source": [
        "import pandas as pd\n",
        "train = pd.read_csv('reviews_train.csv', header=0, delimiter='\\t', quoting=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-NFB0QmgYOn"
      },
      "source": [
        "We can now examine the data. Explore the dataset using the commands below. Note that negative sentiment is denoted by `0` and positive sentiment by `1`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-qFU3yc3sXb"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaqbbOI53wqS"
      },
      "source": [
        "print(train['review'][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCiAJTJ7g4ky"
      },
      "source": [
        "Generally, we need to preprocess the dataset to be able to properly extract features from it. Within a computer, text is encoded as a continuous string of characters. For a computer, there is no difference between a space, a punctuation mark, or an alpha-numeric character. This means that a computer does not know about word or sentence boundaries. In order to analyze textual data within natural language processing applications, we first need to properly preprocess it. Note that our data has already been tokenized (i.e. words have been properly segmented), so we don't need to preprocess our data anymore.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0CHatcRjCPF"
      },
      "source": [
        "### Feature extraction\n",
        "\n",
        "Now it’s time to decide which features to use in our classifier. We’ll start with simple bag of words features. For this, we can make use of `scikit-learn`'s `CountTokenizer` object. `scikit-learn` (the model name of which is `sklearn`) is a well known Python library that implements a wide range of machine learning algorithms. `CountVectorizer` is part of `scikit-learn`'s feature extraction toolkit; it allows us to convert a collection of documents to a matrix of token counts (one feature vector for each review). We define the parameter `max_features` to be 500, which means only the 500 most frequent words will be taken into account. Once we have defined the `CountVectorizer` object, we can perform the actual feature extraction using the function `fit_transform`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQDGYrPI5B0_"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer(analyzer = 'word', max_features = 500)\n",
        "train_data_features = vectorizer.fit_transform(train['review'])\n",
        "\n",
        "# train_data_features is a big vector containing for each review, the amount of times a certain word is present. The words are represented as numbers, which the CountVectorizer did.\n",
        "\n",
        "# With fit_transform, you match the occurence of each word to the sentiment? vb, if a negative sentiment is linked to the word 'terrible' occurring 10 times, then terrible is probably a negative word."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMdicabMmEaj"
      },
      "source": [
        "Using the command below, we can inspect the shape of the resulting matrix:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TObnhPo25PG5"
      },
      "source": [
        "print(train_data_features.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gA7rVyYJmjkK"
      },
      "source": [
        "The output above tells us that we have a matrix of 20000 documents (reviews), by 500 word features. Each matrix cell represents the frequency of each of the 500 words within a particular review.\n",
        "\n",
        "We can also inspect which are the 500 most frequent words that are used as features, using the command below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Af5Unv1WmSig"
      },
      "source": [
        "vocab = vectorizer.get_feature_names_out()\n",
        "print(vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tiuNsgTnxag"
      },
      "source": [
        "### Classification\n",
        "\n",
        "`scikit-learn` contains many different implementations of classification algorithms. We’ll start with the classifier we examined during the lectures: a naive Bayes classifier. The naive Bayes classifier is represented by the `MultinomialNB` class. Once we have instantiated it, we can train it on our data using the `fit` function. Note that we are providing to the function both the features extracted for each review (`train_data_features`) and the correct sentiment label (`train['sentiment']`) for each review. In the training step, we need to provide our model with the correct answer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQgr9LGj5WAR"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "classifier = MultinomialNB()\n",
        "\n",
        "classifier.fit(train_data_features, train['sentiment'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxRU0QIUpyXI"
      },
      "source": [
        "Our model has now been trained on the training set; we can now test its performance on the test set. First, we carry out the same preprocessing and feature extraction on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBSJGwz-5acV"
      },
      "source": [
        "test = pd.read_csv('reviews_test.csv', header=0, delimiter='\\t', quoting=3)\n",
        "test_data_features = vectorizer.transform(test['review'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsHAV8S_p_PV"
      },
      "source": [
        "Next, we can compute the performance on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6t2_oh25_pQ"
      },
      "source": [
        "score = classifier.score(test_data_features, test['sentiment'])\n",
        "print(score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWtLwBMJqrWi"
      },
      "source": [
        "#### Exercise 1\n",
        "\n",
        "\n",
        "\n",
        "1.   What does the score represent?\n",
        "2.   Look at the instances that were classified badly. Do you see why certain reviews are misclassified? Hint: use the function `predict`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_XFvn45qGD2"
      },
      "source": [
        "## The score represents the average accuracy of the model in predicting sentiment on new data.\n",
        "\n",
        "# The predict function (on the fitted model) gives the predicted sentiment of each review in the input data.\n",
        "predicted = classifier.predict(test_data_features)\n",
        "# Compare performance on test set with the actual sentiments:\n",
        "sentiments = train['sentiment']\n",
        "comparison = predicted - sentiments[len(predicted)]\n",
        "\n",
        "### VERBETERING ###\n",
        "\n",
        "import numpy as np\n",
        "predictions = classifier.predict(test_data_features)\n",
        "\n",
        "np.where(predictions != test['sentiment']) # check where predicted and correct ones differ\n",
        "test['review'][3]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krzzw_QJrWnG"
      },
      "source": [
        "### K-fold cross validation\n",
        "\n",
        "Usually, we will want to try out different parameters, in order to see what works best for our task. As such, we might experiment with:\n",
        "\n",
        "\n",
        "* Different features\n",
        "* Different classification algorithms\n",
        "* Different model parameters\n",
        "\n",
        "\n",
        "However, we have to be careful: we cannot use our test set over and over again, as we'll be optimizing our parameters for that particular test set (and run the risk of overfitting, which means we are not able to properly generalize to data we haven't trained on). For this reason, we need to make use of a **validation set**. However, our training set is already quite small; creating a separate validation set would give us even less training data. Fortunately, we don't have to create a separate set: we can use **k-fold cross validation**. The idea is the following:\n",
        "\n",
        "* Break up data into $k$ (e.g. 10) parts (folds)\n",
        "* For each fold\n",
        "\n",
        "    * Current fold is used as temporary test set (called **validation** set)\n",
        "    * Use other 9 folds as training data\n",
        "    * Performance is computed on test fold\n",
        "\n",
        "*  Average performance over 10 runs\n",
        "\n",
        "A graphical representation of 10-fold cross validation is given below:\n",
        "\n",
        "\n",
        "![10_fold_cv.png](data:image/png;base64,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)\n",
        "\n",
        "K-fold cross validation is implemented in the `StratifiedKFold` class. 'Stratified' means that we keep the number of positive and negative instances in the train and test set balanced throughout the cross-validation steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtfMLI-srZ-5"
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "kf = StratifiedKFold(n_splits=10) # partition the sentiments equally over all folds.\n",
        "score_kfold = []\n",
        "\n",
        "# what the loop does: we have the folds, and want to train data on each one.\n",
        "for n_fold, (train_indices, test_indices) in enumerate(kf.split(train_data_features, train['sentiment'])):\n",
        "    X_train, X_test = train_data_features[train_indices], train_data_features[test_indices]\n",
        "    y_train, y_test = train['sentiment'][train_indices], train['sentiment'][test_indices]\n",
        "\n",
        "    classifier.fit(X_train, y_train)\n",
        "    print('Score on fold' , n_fold, ':', classifier.score(X_test, y_test))\n",
        "    score_kfold.append(classifier.score(X_test, y_test))\n",
        "\n",
        "print('Average performance: ', sum(score_kfold) / len(score_kfold))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILUBrm-bxshZ"
      },
      "source": [
        "#### Exercise 2\n",
        "\n",
        "* Experiment with different feature sets\n",
        "    * Experiment with n-grams instead of bag of words (hint: look at the arguments of CountVectorizer again in order to extract n-grams)\n",
        "\n",
        "    * Change the number of vocabulary elements included in the model\n",
        "\n",
        "* Experiment with different models\n",
        "\n",
        "    * Try [a naive bayes classifier that uses binary features](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html) (word presence\n",
        "instead of word count)\n",
        "    * Try [any other classifier included with scikit-learn](https://scikit-learn.org/stable/supervised_learning.html) (e.g. [decision trees](https://scikit-learn.org/stable/modules/tree.html), or [logistic regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression)) How does it perform?\n",
        "\n",
        "* When you’ve determined the best set of parameters (according to cross-validation), compute the performance on the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mn6PQDRH0tR7"
      },
      "source": [
        "######## Experiment with different feature sets ############\n",
        "\n",
        "### feature extraction ###\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# We now tell the CountVectorizer to not count words, but n-grams: these are sequences of n tokens (in this case words?) from the dataset.\n",
        "# For instance, one token may be 'the big', another 'big cat'. This way, it is easier for the model to spot words that are often placed consequentially?\n",
        "vectorizer = CountVectorizer(analyzer = 'word', ngram_range=(2,3), max_features = 1000)\n",
        "train_data_features = vectorizer.fit_transform(train['review'])\n",
        "\n",
        "### classification ###\n",
        "\n",
        "### wrong: did not do k fold validation. See correction.\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "classifier = MultinomialNB()\n",
        "\n",
        "classifier.fit(train_data_features, train['sentiment'])\n",
        "\n",
        "### training: preparing test set ###\n",
        "\n",
        "test = pd.read_csv('reviews_test.csv', header=0, delimiter='\\t', quoting=3)\n",
        "test_data_features = vectorizer.transform(test['review'])\n",
        "\n",
        "### testing accuracy of model ###\n",
        "\n",
        "score = classifier.score(test_data_features, test['sentiment'])\n",
        "print(score)\n",
        "\n",
        "# After playing around with the # of n-grams and # of features, I find:\n",
        "        # the higher the # of features (>1000), the higher the accuracy.\n",
        "        # the lower the length of n-grams (1,2), the higher the accuracy.\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######## Experiment with different models ############\n",
        "#### naive Bayes binary classifier ####\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "\n",
        "### feature selection ###\n",
        "vectorizer2 = CountVectorizer(binary= True) # This time, we create binary features: if the word is present or not.\n",
        "train_data_features2 = vectorizer2.fit_transform(train['review'])\n",
        "\n",
        "### classification ###\n",
        "classifier2 = BernoulliNB()\n",
        "sentiments = train['sentiment']\n",
        "classifier2.fit(train_data_features2, sentiments)\n",
        "\n",
        "### Training ###\n",
        "test = pd.read_csv('reviews_test.csv', header=0, delimiter='\\t', quoting=3)\n",
        "test_data_features = vectorizer.transform(test['review'])\n",
        "\n",
        "### Testing accuracy of model ###\n",
        "score = classifier2.score(test_data_features, test['sentiment'])\n",
        "print(score)\n"
      ],
      "metadata": {
        "id": "Rirwfo0Y-zBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######## Experiment with different models ############\n",
        "#### other classifier (decision tree or logistic regression for instance) ####\n",
        "from sklearn\n",
        "\n",
        "\n",
        "### conclusion: logistic regression is best out of all models.\n"
      ],
      "metadata": {
        "id": "yyOA1-0WEvi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYbXAMQKyf70"
      },
      "source": [
        "### Intrinsic model evaluation\n",
        "\n",
        "Some models allow us to look at the most informative features. Using a logistic\n",
        "regression, you can do the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXqmB5xBxtpZ"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "classifier = LogisticRegression(max_iter=1000)\n",
        "classifier.fit(train_data_features, train['sentiment'])\n",
        "\n",
        "allCoefficients = [(classifier.coef_[0,i], vocab[i]) for i in range(len(vocab))]\n",
        "\n",
        "allCoefficients.sort()\n",
        "allCoefficients.reverse()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkmKY6dFzBlP"
      },
      "source": [
        "#### Exercise 3\n",
        "\n",
        "Examine the top and the bottom features of the `allCoefficients` list. Which features are the most informative?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXz4Jfd1yabF"
      },
      "source": [
        "###your code here\n",
        "\n",
        "print(allCoefficients[0:10])\n",
        "print(allCoefficients[-10:-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BL6nDE49RUqi"
      },
      "source": [
        "### References\n",
        "\n",
        "Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. Learning Word Vectors for Sentiment Analysis. The 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011)."
      ]
    }
  ]
}